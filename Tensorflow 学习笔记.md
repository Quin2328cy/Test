# 2.5
## Loss Function
```
在传统的二次cost function中,会出现误差越大,梯度下降越慢的情况,其速度最快是误差处于中间位置的情况
|
| 改进
|
交叉熵:当误差越大时,训练时的梯度也就越大,从而会加快训练的速度
参考[交叉熵代价函数](http://blog.csdn.net/u014313009/article/details/51043064)
```
